import jieba
import pandas as pd
from gensim import corpora
from tensorflow import keras
from sklearn.preprocessing import LabelEncoder
from gensim.models.doc2vec import Doc2Vec,LabeledSentence

file_path="C:\Python\spce\yask\gongdan.csv"
stopwords_path="C:\Python\spce\yask\stopword.txt"
model_path=""

def chinese_word_cut(text):
    return ' '.join(jieba.cut(text))

def load_stopword():
    f_stop=open(stopwords_path)
    sw=[line.strip() for line in f_stop]
    f_stop.close()
    return sw

if __name__ == '__main__':
    stop_words = load_stopword()
    data=pd.read_csv(file_path,encoding='gb18030',nrows=1000)
    data['content_cuted']=data['工单详情'].apply(chinese_word_cut)
    data['content_cuted'] = [[word for word in line.strip().lower().split() if word not in stop_words] for line in data['content_cuted']]

    M = len(data['content_cuted'])

    print('文本数目：%d个' % M)
#将文本转换为BOW表示
    dictionary = corpora.Dictionary(data['content_cuted'])
    V = len(dictionary)
    print('词的个数：', V)
    corpus = [dictionary.doc2bow(text) for text in data['content_cuted']]

#DOC2VEC的向量表示
    # class LabeledLineSentence(object):
    #     def __init__(self, filename):
    #         self.filename = filename
    #     def __iter__(self):
    #         for uid, line in enumerate(data['content_cuted']):
    #             yield LabeledSentence(words=line, tags=['SENT_%s' % uid])
    #
    # sentences=LabeledLineSentence('')
    # model=Doc2Vec(sentences,size=100,window=3,min_count=1)
    # # model.save(model_path)
    # model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)
    model=keras.models.Sequential()
    model.add(keras.layers.Embedding(V, 24, input_length=50))
    model.add(keras.layers.LSTM(64))
    model.add(keras.layers.Dropout(0.5))
    model.add(keras.layers.Dense(9,activation='sigmoid'))
    sgd=keras.optimizers.SGD(lr=0.01,nesterov=True)

    model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])

    texts=data['content_cuted']
    encoded_docs = [keras.preprocessing.text.one_hot(str(d), V) for d in texts]
    max_length = 50
    x_train =  keras.preprocessing.sequence.pad_sequences(encoded_docs, maxlen=max_length, padding='post')
    encoder = LabelEncoder()
    encoded_Y = encoder.fit_transform(data['工单大类'])
    y_train=keras.utils.to_categorical(encoded_Y,num_classes=9)
    model.fit(x_train, y_train, batch_size=16, epochs=5)


